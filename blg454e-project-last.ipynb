{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image as kp_image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Style Transfer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-40110aad1631>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mstyle_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://storage.googleapis.com/kaggle-datasets-images/133/275/a6c2e389f50dfb5bbc76705e893a6c10/dataset-card.png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mstyle_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_img_from_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-40110aad1631>\u001b[0m in \u001b[0;36mload_img_from_url\u001b[1;34m(url, img_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the images from URLs and resize to a fixed size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_img_from_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLANCZOS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the images from URLs and resize to a fixed size\n",
    "def load_img_from_url(url, img_size=(512, 512)):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize(img_size, Image.LANCZOS)\n",
    "    img = kp_image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "style_url = \"https://storage.googleapis.com/kaggle-datasets-images/133/275/a6c2e389f50dfb5bbc76705e893a6c10/dataset-card.png\"\n",
    "\n",
    "style_image = load_img_from_url(style_url)\n",
    "\n",
    "\n",
    "# Load the images from file paths and resize to a fixed size\n",
    "def load_img_from_path(path, img_size=(512, 512)):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize(img_size, Image.LANCZOS)\n",
    "    img = kp_image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Use keras utility to download images\n",
    "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "\n",
    "content_image = load_img_from_path(content_path)\n",
    "\n",
    "\n",
    "\n",
    "# Display the images\n",
    "def imshow(img, title=None):\n",
    "    out = np.squeeze(img, axis=0)\n",
    "    out = out.astype('uint8')\n",
    "    plt.imshow(out)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(out)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image, 'Style Image')\n",
    "plt.show()\n",
    "\n",
    "# Preprocessing for VGG19 model\n",
    "def preprocess_image(img):\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = img.reshape((img.shape[1], img.shape[2], 3))\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img\n",
    "\n",
    "# Load VGG19 model\n",
    "vgg = VGG19(include_top=False, weights='imagenet')\n",
    "vgg.trainable = False\n",
    "\n",
    "# Content and Style layers\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)\n",
    "\n",
    "# Model for extracting style and content\n",
    "def get_model():\n",
    "    style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "    content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "    model_outputs = style_outputs + content_outputs\n",
    "    return Model(inputs=vgg.input, outputs=model_outputs)\n",
    "\n",
    "# Compute content loss\n",
    "def get_content_loss(base_content, target):\n",
    "    return tf.reduce_mean(tf.square(base_content - target))\n",
    "\n",
    "# Compute style loss\n",
    "def gram_matrix(input_tensor):\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "    return gram / tf.cast(n, tf.float32)\n",
    "\n",
    "def get_style_loss(base_style, gram_target):\n",
    "    height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target))\n",
    "\n",
    "# Compute total loss\n",
    "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):    \n",
    "    style_weight, content_weight = loss_weights\n",
    "    \n",
    "    # Feed our init image through our model. This will give us the content and\n",
    "    # style representations at our desired layers.\n",
    "    model_outputs = model(init_image)\n",
    "    \n",
    "    style_output_features = model_outputs[:num_style_layers]\n",
    "    content_output_features = model_outputs[num_style_layers:]\n",
    "    \n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "    \n",
    "    # Accumulate style losses from all layers\n",
    "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "    for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "        style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n",
    "    \n",
    "    # Accumulate content losses from all layers\n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    for target_content, comb_content in zip(content_features, content_output_features):\n",
    "        content_score += weight_per_content_layer * get_content_loss(comb_content[0], target_content)\n",
    "    \n",
    "    # Combine the style and content losses\n",
    "    style_score *= style_weight\n",
    "    content_score *= content_weight\n",
    "    \n",
    "    # Compute the total loss\n",
    "    total_loss = style_score + content_score\n",
    "    return total_loss, style_score, content_score\n",
    "\n",
    "# Extract style and content features\n",
    "def get_feature_representations(model, content_image, style_image):\n",
    "    content_image = preprocess_image(content_image)\n",
    "    style_image = preprocess_image(style_image)\n",
    "    \n",
    "    stack_images = tf.concat([content_image, style_image], axis=0)\n",
    "    features = model(stack_images)\n",
    "    \n",
    "    style_features = features[:num_style_layers]\n",
    "    content_features = features[num_style_layers:]\n",
    "    \n",
    "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "    \n",
    "    return gram_style_features, content_features\n",
    "\n",
    "# Style transfer using gradient descent\n",
    "def compute_grads(cfg):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_loss = compute_loss(**cfg)\n",
    "    total_loss = all_loss[0]\n",
    "    return tape.gradient(total_loss, cfg['init_image']), all_loss\n",
    "\n",
    "# Main function to perform style transfer\n",
    "def run_style_transfer(content_image, style_image, num_iterations=1000, content_weight=1e4, style_weight=1e-2):\n",
    "    model = get_model() \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    style_features, content_features = get_feature_representations(model, content_image, style_image)\n",
    "    init_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "    opt = tf.optimizers.Adam(learning_rate=5.0)\n",
    "    \n",
    "    iter_count = 1\n",
    "    best_loss, best_img = float('inf'), None\n",
    "    loss_weights = (style_weight, content_weight)\n",
    "    \n",
    "    cfg = {\n",
    "        'model': model,\n",
    "        'loss_weights': loss_weights,\n",
    "        'init_image': init_image,\n",
    "        'gram_style_features': style_features,\n",
    "        'content_features': content_features\n",
    "    }\n",
    "    \n",
    "    norm_means = np.array([103.939, 116.779, 123.68])\n",
    "    min_vals = -norm_means\n",
    "    max_vals = 255 - norm_means\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, all_loss = compute_grads(cfg)\n",
    "        loss, style_score, content_score = all_loss\n",
    "        opt.apply_gradients([(grads, init_image)])\n",
    "        clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
    "        init_image.assign(clipped)\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_img = deprocess_image(init_image.numpy())\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration: {}'.format(i))\n",
    "            print('Total loss: {:.4e}, Style loss: {:.4e}, Content loss: {:.4e}'.format(loss, style_score, content_score))\n",
    "    \n",
    "    return best_img, best_loss\n",
    "\n",
    "\n",
    "best, best_loss = run_style_transfer(content_image, \n",
    "                                     style_image, num_iterations=1000 )\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(best)\n",
    "plt.title('Output Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction Using Pre-trained Model (GoogleNet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load GoogleNet model\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg')\n",
    "base_model.trainable = False\n",
    "\n",
    "def extract_features(image):\n",
    "    preprocessed_image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
    "    features = base_model.predict(preprocessed_image)\n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "content_feature = extract_features(content_image)\n",
    "stylized_feature = extract_features(np.expand_dims(best, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Contrastive Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positive and negative pairs\n",
    "positive_pair = (content_feature, stylized_feature)\n",
    "negative_pair = (content_feature, content_feature)  # Simulating a negative pair for the sake of the example\n",
    "\n",
    "def contrastive_loss(features1, features2, positive=True, temperature=0.1):\n",
    "    similarity = tf.keras.losses.cosine_similarity(features1, features2)\n",
    "    if positive:\n",
    "        return -tf.math.log(tf.math.exp(-similarity) / (tf.math.exp(-similarity) + tf.math.exp(similarity)))\n",
    "    else:\n",
    "        return -tf.math.log(tf.math.exp(similarity) / (tf.math.exp(similarity) + tf.math.exp(-similarity)))\n",
    "\n",
    "# Compute contrastive loss\n",
    "positive_loss = contrastive_loss(positive_pair[0], positive_pair[1], positive=True)\n",
    "negative_loss = contrastive_loss(negative_pair[0], negative_pair[1], positive=False)\n",
    "\n",
    "total_loss = positive_loss + negative_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freezing the Trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n"
     ]
    }
   ],
   "source": [
    "# Simulate additional data (data augmentation)\n",
    "def augment_image(image):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "    ])\n",
    "    return data_augmentation(image)\n",
    "\n",
    "augmented_images = [augment_image(content_image) for _ in range(10)]\n",
    "augmented_features = [extract_features(augmented_image) for augmented_image in augmented_images]\n",
    "\n",
    "# Prepare training data\n",
    "X_train = np.vstack((content_feature, stylized_feature, *augmented_features))\n",
    "y_train = np.hstack((0, 1, *[0]*len(augmented_features)))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a Linear Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdinlinezrin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9202 - loss: 0.6196  \n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9762 - loss: 0.2638  \n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8131 - loss: 0.5108 \n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9202 - loss: 0.2097 \n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9619 - loss: 0.1046 \n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9202 - loss: 0.1276 \n",
      "Epoch 7/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8845 - loss: 0.1497 \n",
      "Epoch 8/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.1499 \n",
      "Epoch 9/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 1.0000 - loss: 0.1887  \n",
      "Epoch 10/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0997 \n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x00000275912DA5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.0807\n",
      "Classifier accuracy on training set: 100.00%\n",
      "Iteration: 0\n",
      "Total loss: 2.9797e+08, Style loss: 2.6176e+08, Content loss: 3.6213e+07\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.4788\n",
      "Classifier accuracy on test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Create and train a linear classifier model\n",
    "classifier_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "classifier_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the classifier\n",
    "classifier_model.fit(X_train, y_train, epochs=10, batch_size=2)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "loss_train, accuracy_train = classifier_model.evaluate(X_train, y_train)\n",
    "\n",
    "print(f'Classifier accuracy on training set: {accuracy_train * 100:.2f}%')\n",
    "\n",
    "# Load the content image for the test set\n",
    "content_path_test = tf.keras.utils.get_file('LabradorRetriever.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "content_image_test = load_img_from_path(content_path_test)\n",
    "\n",
    "# Perform style transfer on the test content image\n",
    "best_test, best_loss_test = run_style_transfer(content_image_test, style_image, num_iterations=100)\n",
    "\n",
    "# Extract features for the test content and stylized images\n",
    "content_feature_test = extract_features(content_image_test)\n",
    "stylized_feature_test = extract_features(np.expand_dims(best_test, axis=0))\n",
    "\n",
    "# Prepare the test data\n",
    "X_test = np.vstack((content_feature_test, stylized_feature_test))\n",
    "y_test = np.array([0, 1])  # Ground truth labels for the test data\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_test, accuracy_test = classifier_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Classifier accuracy on test set: {accuracy_test * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0807\n",
      "Classifier accuracy: 100.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = classifier_model.evaluate(X_train, y_train)\n",
    "\n",
    "print(f'Classifier accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n",
    "y_pred = classifier_model.predict(X_train).round()\n",
    "precision = precision_score(y_train, y_pred)\n",
    "recall = recall_score(y_train, y_pred)\n",
    "f1 = f1_score(y_train, y_pred)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
